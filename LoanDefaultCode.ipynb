{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Logistic Regression to predict the likelihood of a loan default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                                                                       Tanya Naheta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This report provides an anaylsis of mortgage data in order to predict the likelihood of a loan default. A variety of methods will be used to analyze and interpret the data in order to build a model based on logistic regression. I investigated the logistic regression algorithm, fine tuned it to the provided data set and provided results using the training data set. The results from this report show that the model was quite successful at predicting the likelihood of a loan default. In doing the analysis of the provided data, I also discovered that interest rate, debt to income ratio and the original loan-to-value are some attributes that are higly correlated to the likelihood of a loan default meaning they were important features in building the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting whether or not a loan will default can seem very daunting given that it likely depends on a variety of factors. However, if we were to isolate a few key factors and characteristics of a loan, we could likely build a model that accurately predicts whether or not a loan will default. This report will dive deeper into this exploration and try to highlight the key variables that closely correlate to the likelihood of a loan default.\n",
    "\n",
    "These are the attributes available to us in the dataset:\n",
    "\n",
    "* Channel       \n",
    "* SellerName \n",
    "* OriginalInterestRate\n",
    "* OriginalUnpaidPrincipal\n",
    "* OriginalLoanTerm\n",
    "* OriginationDate\n",
    "* FirstPayment\n",
    "* OriginalLoanToValue\n",
    "* OriginalCombinedLoanToValue\n",
    "* NumBorrowers\n",
    "* DebtToIncomeRatio\n",
    "* CreditScore\n",
    "* FirstTimeHomeBuyer\n",
    "* LoanPurpose\n",
    "* PropertyType\n",
    "* NumUnits\n",
    "* OccupancyStatus\n",
    "* PropertyState\n",
    "* Zip\n",
    "* MortgageInsurancePercentage\n",
    "* ProductType\n",
    "* CoBorrowerCreditScore\n",
    "* MortgageInsuranceType\n",
    "* RelocationMortgageIndicator\n",
    "* ForeclosureDate: Valid date if the home was foreclosed otherwise null (Will be used as a target variable)\n",
    "\n",
    "The goal of this anaylsis is to be able to accurately predict the loan default variable given the attributes provided to our algorithm. \n",
    "\n",
    "There are a couple of hypotheses that will be tested during this anaylsis as well. One of the hypothesis regarding this dataset is that the credit score will have an inverse correlation to the likelihood of a loan default. This is because people with a higher credit score should be in a more financially stable position and less likely to default. Another hypothesis is that the original interest rate on the loan will have a strong correlation to the likelihood of a loan default. This is because a higher interest rate would mean that borrowers have to pay off more money, making it harder for them to do so. We will be testing both of these hypotheses by comparing the aforementioned attributes with our target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "acquisition_columns = ['LoanID','Channel','SellerName','OrInterestRate','OrUnpaidPrinc','OrLoanTerm',\n",
    "        'OrDate','FirstPayment','OrLTV','OrCLTV','NumBorrow','DTIRat','CreditScore',\n",
    "        'FTHomeBuyer','LoanPurpose','PropertyType','NumUnits','OccStatus','PropertyState',\n",
    "        'Zip','MortInsPerc','ProductType','CoCreditScore','MortInsType','RelMortInd'];\n",
    "\n",
    "performance_columns = ['LoanID','MonthRep','Servicer','CurrInterestRate','CAUPB','LoanAge','MonthsToMaturity',\n",
    "          'AdMonthsToMaturity','MaturityDate','MSA','CLDS','ModFlag','ZeroBalCode','ZeroBalDate',\n",
    "          'LastInstallDate','ForeclosureDate','DispositionDate','PPRC','AssetRecCost','MHRC',\n",
    "          'ATFHP','NetSaleProceeds','CreditEnhProceeds','RPMWP','OFP','NIBUPB','PFUPB','RMWPF',\n",
    "          'FPWA','ServicingIndicator'];\n",
    "\n",
    "acquisition_dataframe = pd.read_csv('Acquisition_2008Q1.txt', sep='|', names=acquisition_columns, index_col=False)\n",
    "performance_dataframe = pd.read_csv('Performance_2008Q1.txt', sep='|', names=performance_columns, index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Any duplicate Loan ID's in performance dataset?\", performance_dataframe['LoanID'].duplicated().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Any duplicate Loan ID's in acquisition dataset?\", acquisition_dataframe['LoanID'].duplicated().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are duplicate Loan ID's in the performance dataset, we will need to take the latest entry so we get the most up to date information. We can remove all duplicates from the performance dataset as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_dataframe.drop_duplicates(subset=\"LoanID\", keep=\"last\", inplace=True)\n",
    "print(\"Any duplicate Loan ID's in performance dataset?\", performance_dataframe['LoanID'].duplicated().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have ensured that there are no duplicates in the performance dataset by keeping only the last and most up to date entry for the Loan ID's. Now, we can merge the two datasets together in order to be able to train our model. Since we are going to be predicting whether or not a loan will default, we will mainly use the acquisition dataset attributes to train our model. However, we need the target variable (whether the loan defaulted or not) from the performance dataset. So, we are going to drop the rest of the performance columns and merge only the 'ForeclosureDate' column based on the Loan ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all columns from performance dataframe apart from LoanID and ForeclosureDate\n",
    "performance_dataframe = performance_dataframe.filter(['LoanID', 'ForeclosureDate'])\n",
    "\n",
    "# Merge the two dataframes together\n",
    "df = pd.merge(acquisition_dataframe, performance_dataframe, on='LoanID')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our combined dataframe that we can use to train our model! First, we need to fill in the missing values for ForeclosureDate and turn it into our target variable column. In order to do this, we can replace all rows with a foreclosure date with 1 and the rest with 0 (ie. if a foreclosure date is present, the loan was defaulted on.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df[['ForeclosureDate']] = df[['ForeclosureDate']].applymap(lambda x: 0 if pd.isnull(x) else 1)\n",
    "# Change column name since it is now of type boolean\n",
    "df.rename(columns={'ForeclosureDate': 'Foreclosed'}, inplace=True)\n",
    "\n",
    "# Check to see the distribution of the target variable (ForeclosureDate)\n",
    "sns.countplot(df['Foreclosed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have changed the column name and replaced the dates with 1's and 0's, we can plot the target variable to see the spread of the values. From the plot above, we see that the amount of foreclosed homes is very low compared to the amount of homes that were not foreclosed on. This is not ideal since it might incorrectly train our model to skew towards homes that were not foreclosed. In order to correct this and have a balance of these values, we can up-sample our minority class to ensure our model does not skew one way or another. (Chawla, 326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find exact numbers of target variable distribution\n",
    "df['Foreclosed'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate the majority and minority classes for our target variable\n",
    "df_majority = df[df.Foreclosed==0]\n",
    "df_minority = df[df.Foreclosed==1]\n",
    "\n",
    "# For n_samples, we can pass in the number of loans that have Foreclosed set to 0. \n",
    "# This way we will have a balance between the values for our target variable.\n",
    "df_upsampled_minority = resample(df_minority, \n",
    "                                 replace=True,\n",
    "                                 n_samples=354708,\n",
    "                                 random_state=123)\n",
    "\n",
    "# Create a balanced dataframe\n",
    "df = pd.concat([df_majority, df_upsampled_minority])\n",
    "df['Foreclosed'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a balanced distribution for the target variable so we can continue analyzing the rest of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the amount of null values by column\n",
    "df.isnull().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the numbers above, we can see that MortInsPerc, MortInsType and CoCreditScore have a high number of null values so we can drop these columns from our dataframe since they will not be useful to train the model. We can also remove the LoanID column since that is just used as an identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop some columns that have null values and LoanID (not useful in our analysis)\n",
    "df.drop(['MortInsPerc','CoCreditScore','MortInsType','LoanID'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate correlation matrix (measures correlation between attributes and target variable)\n",
    "corrMatrix = df.corr()\n",
    "plt.subplots(figsize=(20,10))\n",
    "cmap = sns.diverging_palette(0,255,sep=10, as_cmap=True)\n",
    "sns.heatmap(corrMatrix, xticklabels=corrMatrix.columns, yticklabels=corrMatrix.columns, annot=True, cmap=cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get a general overview of the data, I plotted a correlation matrix (Numpacharoen, 1), which measures the correlation between the different attributes that we have and the target variable. From the matrix above, we can see that some of the highest correlation to the target variable exists in the columns: OrInterestRate, OrLTV, OrCLTV, OrLoanTerm. Also, as expected, CreditScore seems to have a negative correlation to the target variable of Foreclosed. The correlation matrix above provides a high level overview of our attributes, we can dive deeper into the data and compare the different attributes we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "columns = ['DTIRat','CreditScore','OrInterestRate', 'OrLTV']\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(10,10))\n",
    "\n",
    "for i, y in zip([1,2,3,4], columns):\n",
    "    plt.subplot(2,2,i)\n",
    "    sns.boxplot(x=\"Foreclosed\", y=y, data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was worth exploring the possibility of covariance between the credit score and interest rate variables as it is likely that someone with a low credit score will be charged a high interest rate on their loan. I plotted these two variables on a scatter plot, however, there doesn’t seem to be much correlation between the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['OrInterestRate']\n",
    "y = df['CreditScore']\n",
    "plt.scatter(x, y)\n",
    "plt.title(\"Interest Rate vs. Credit Score\")\n",
    "plt.xlabel(\"Interest Rate\")\n",
    "plt.ylabel(\"Credit Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can map the distribution of the credit score along with the likelihood of default at various levels. From the plot below, we can see very clearly that having a lower credit score leads to a higher number of defaults, which is in line with what we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_credit_score = df.loc[df['Foreclosed'] == 1, 'CreditScore']\n",
    "non_default_credit_score = df.loc[df['Foreclosed'] == 0, 'CreditScore']\n",
    "\n",
    "plt.figure(figsize=(8,6), edgecolor='blue')\n",
    "plt.hist([default_credit_score, non_default_credit_score], np.linspace(200, 800, 30), label=['Default', 'No Default'],)\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel(\"Credit Score\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can map the distribution of the debt to income ratio along with the likelihood of default at various levels. From the plot below, we can see that having a higher debt to income ratio leads to a higher number of defaults, which is expected since people with higher levels of debt will likely have more trouble paying off the loan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_dti_ratio = df.loc[df['Foreclosed'] == 1, 'DTIRat']\n",
    "non_default_dti_ratio = df.loc[df['Foreclosed'] == 0, 'DTIRat']\n",
    "\n",
    "plt.figure(figsize=(8,6), edgecolor='blue')\n",
    "plt.hist([default_dti_ratio, non_default_dti_ratio], np.linspace(0, 80, 10), label=['Default', 'No Default'],)\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel(\"DTI Ratio\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can map the distribution of the original interest rate along with the likelihood of default at various levels. From the plot below, we can see that having a higher interest rate leads to a higher number of defaults, which is expected since people with higher interest rates will likely have more trouble paying off the loan due to the extra interest that gets accumulated as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_ir = df.loc[df['Foreclosed'] == 1, 'OrInterestRate']\n",
    "non_default_ir = df.loc[df['Foreclosed'] == 0, 'OrInterestRate']\n",
    "\n",
    "plt.figure(figsize=(8,6), edgecolor='blue')\n",
    "plt.hist([default_ir, non_default_ir], np.linspace(0, 9, 10), label=['Default', 'No Default'],)\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel(\"Interest Rate\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ProductType'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop ProductType since it has only one value\n",
    "df.drop(['ProductType'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 10 columns where the types are not numerical so we need to alter those in order to be able to feed that data to our model. We will do that using a variety of techniques. For the OrDate and FirstPayment columns, they are input as month/year and so we can split those up into two separate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMonth (record):\n",
    "    return record.split('/')[0].strip()\n",
    "\n",
    "def getYear (record):\n",
    "    return record.split('/')[1].strip()\n",
    "df['OrMonth'] = df['OrDate'].apply(lambda record: getMonth(record))\n",
    "df['OrYear'] = df['OrDate'].apply(lambda record: getYear(record))\n",
    "\n",
    "df['FirstPayment_Month'] = df['FirstPayment'].apply(lambda record: getMonth(record))\n",
    "df['FirstPayment_Year'] = df['FirstPayment'].apply(lambda record: getYear(record))\n",
    "\n",
    "df[['OrMonth', 'OrYear', 'FirstPayment_Month', 'FirstPayment_Year']] = df[['OrMonth', 'OrYear', 'FirstPayment_Month', 'FirstPayment_Year']].apply(pd.to_numeric)\n",
    "\n",
    "# Drop the original columns since we have split them up into month and year\n",
    "df.drop(['OrDate','FirstPayment'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the columns with object type\n",
    "df = pd.get_dummies(df, columns=['Channel', 'SellerName', 'FTHomeBuyer', 'LoanPurpose', 'PropertyType', 'OccStatus', 'PropertyState', 'RelMortInd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill columns that had null values with the mode values of those columns\n",
    "df = df.fillna(df['DTIRat'].value_counts().index[0])\n",
    "df = df.fillna(df['NumBorrow'].value_counts().index[0])\n",
    "df = df.fillna(df['CreditScore'].value_counts().index[0])\n",
    "df = df.fillna(df['OrInterestRate'].value_counts().index[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now pre-process the data set so that our models can be trained better. We will normalize it so that all attributes have values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale all the data we have so it's between the 0 and 1 range\n",
    "from sklearn import preprocessing\n",
    "df_minmax = preprocessing.MinMaxScaler().fit(df)\n",
    "df = pd.DataFrame(df_minmax.transform(df), index=df.index, columns=df.columns)\n",
    "df_normalized = pd.DataFrame(df, columns=df.columns, index=df.index)\n",
    "df_normalized.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to build the algorithms, we are going to split the training data into a train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# assign all but Foreclosed data columns to X and the Foreclosed to y since that is our target\n",
    "X = df_normalized.drop(['Foreclosed'], axis=1).values\n",
    "y = df_normalized['Foreclosed'].values\n",
    "\n",
    "# create train-test split for the data we have to train and tune models\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Find best parameters for Logistic regression (Vabalas, 8)\n",
    "params_lr = { 'penalty':['l1', 'l2', 'elasticnet', 'none'],\n",
    "         'C':[0.01,0.1,1,10,100],\n",
    "         'max_iter': [50, 100, 200, 500, 1000],\n",
    "         'class_weight':['balanced',None]}\n",
    "logistic_regression = GridSearchCV(LogisticRegression(), param_grid=params_lr, n_jobs=-1)\n",
    "\n",
    "# Train the model we generated\n",
    "logistic_regression.fit(x_train,y_train)\n",
    "logistic_regression.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for the test set\n",
    "prediction = logistic_regression.predict(x_test)\n",
    "\n",
    "# Calculate the accuracy and f1 scores\n",
    "lr_accuracy_score = accuracy_score(y_test,prediction)\n",
    "lr_f1_score = f1_score(y_test, prediction)\n",
    "\n",
    "# Output the confusion matrix and the classification report\n",
    "print(\"\\033[1mConfusion Matrix\\033[0m\\n\")\n",
    "print(confusion_matrix(y_test, prediction))\n",
    "print(\"\\n\\033[1mClassification Report\\033[0m\\n\")\n",
    "print(classification_report(y_test, prediction))\n",
    "\n",
    "# Output the accuracy and f1 scores\n",
    "print(\"Accuracy score: \", round(lr_accuracy_score, 2))\n",
    "print(\"F1 score: \", round(lr_f1_score, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "lr_roc_predictions = logistic_regression.predict_proba(x_test)\n",
    "\n",
    "# calculate area under the curve\n",
    "print(\"Area under curve:\")\n",
    "print(round(roc_auc_score(y_test, lr_roc_predictions[:,1]),2))\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, lr_roc_predictions[:,1])\n",
    "\n",
    "# plot the roc curve\n",
    "plt.clf()\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristics Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy/f1 score plot\n",
    "plt.bar(['Accuracy Score', 'F1 Score'], [accuracy_score, f1_score])\n",
    "plt.title('Logistic regression scores')\n",
    "plt.xlabel('Scores')\n",
    "plt.ylabel('Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After collecting results from all the logistic regression model, we can see analyze them to see how it performed. From the table and graphs above, we can see that model had an accuracy score of 75% and an f1 score of 76%. These numbers are good and were futher backed up by the area under curve metric, which came in at 0.83."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypotheses validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two hypotheses that were set out at the beginning of this analysis were:\n",
    "* Credit score is inversely correlated with our target variable\n",
    "* Original interest rate is correlated with our target variable\n",
    "\n",
    "Both of these hypotheses were proven to be true and the plots during our data exploration show these to be true. A correlation matrix was plotted, which showed the correlation between the attributes and the target variable. These were the values that were seen for the two attributes mentioned above:\n",
    "* Credit score: -0.29\n",
    "* Original interest rate: 0.34\n",
    "\n",
    "These correlation values show that the credit score attribute is inversely correlated with our target variable while the original interest rate attribute is positively correlated with our target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Future Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the future, to improve these results, the categorical columns could be handled slightly differently by analyzing the frequencies of each of the occurences combined with their correlation to the target variable. Also, instead of filling in the missing values with the column mode, we could use another predictive model to fill in the values. Another idea to improve the model would be to compare it to some of the other algorithms (K-nearest neighbors, NBC, Decision tree, etc.) in order to see which model suits this dataset and problem type."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliography\n",
    "\n",
    "Vabalas, Andrius, et al. “Machine Learning Algorithm Validation with a Limited Sample Size.” Plos One, vol. 14, no. 11, 2019, doi:10.1371/journal.pone.0224365.\n",
    "\n",
    "Chawla, N. V., et al. “SMOTE: Synthetic Minority Over-Sampling Technique.” Journal of Artificial Intelligence Research, vol. 16, 2002, pp. 321–357., doi:10.1613/jair.953.\n",
    "\n",
    "Numpacharoen, Kawee, and Amporn Atsawarungruangkit. “Generating Correlation Matrices Based on the Boundaries of Their Coefficients.” PLoS ONE, vol. 7, no. 11, 2012, doi:10.1371/journal.pone.0048902."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
